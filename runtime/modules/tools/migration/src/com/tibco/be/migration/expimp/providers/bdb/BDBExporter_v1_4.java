package com.tibco.be.migration.expimp.providers.bdb;

import java.io.ByteArrayInputStream;
import java.io.ByteArrayOutputStream;
import java.io.DataInputStream;
import java.io.DataOutputStream;
import java.io.File;
import java.io.IOException;
import java.util.Collection;
import java.util.HashMap;
import java.util.Iterator;
import java.util.Properties;

import com.sleepycat.je.Cursor;
import com.sleepycat.je.Database;
import com.sleepycat.je.DatabaseEntry;
import com.sleepycat.je.Environment;
import com.sleepycat.je.EnvironmentConfig;
import com.sleepycat.je.OperationStatus;
import com.tibco.be.dbutils.DbUtils;
import com.tibco.be.dbutils.PropertyTypes;
import com.tibco.be.migration.CSVWriter;
import com.tibco.be.migration.CSVWriterImpl;
import com.tibco.be.migration.expimp.Base64;
import com.tibco.be.migration.expimp.ExpImpContext;
import com.tibco.be.migration.expimp.ExpImpStats;
import com.tibco.be.migration.expimp.WorkerThreadPool;
import com.tibco.be.model.rdf.RDFTypes;
import com.tibco.be.model.util.ModelNameUtil;
import com.tibco.cep.designtime.model.element.Concept;
import com.tibco.cep.designtime.model.element.PropertyDefinition;
import com.tibco.cep.designtime.model.event.Event;
import com.tibco.cep.designtime.model.event.EventPropertyDefinition;
import com.tibco.cep.kernel.service.logging.Level;
import com.tibco.cep.kernel.service.logging.Logger;

/**
 * Created by IntelliJ IDEA.
 * User: ssubrama
 * Date: Feb 18, 2008
 * Time: 8:48:34 AM
 * To change this template use File | Settings | File Templates.
 */
public class BDBExporter_v1_4 {

    public static final String BE_VERSION_RECORD="BEVersionRecord"; // From ObjectManager
    private static final int NUMBER_OF_THREADS = 3;
    private static final int NUMBER_OF_JOBS = 3;

    ExpImpContext context;
    ExpImpStats stats;
    Properties prop;
    Environment dbenv;
    WorkerThreadPool exporterThreadPool;

    private HashMap propIndexToPropInfo = new HashMap();
//    private Set scoreCardSet = new HashSet();
    private HashMap csvCeptWriters = new HashMap();
    private HashMap csvEvtWriters = new HashMap();
    private HashMap evtClsNames = new HashMap();
    private HashMap ceptClsNames = new HashMap();
    private long lastInternalId;
    private String outputPath;

    public BDBExporter_v1_4()
    {
    }

    public void init(ExpImpContext context, ExpImpStats stats) throws Exception
    {
        this.context = context;
        this.stats = stats;

        prop = context.getEnvironmentProperties();
        File dbdir = new File(context.getInputUrl());
        EnvironmentConfig envConfig = new EnvironmentConfig();
        envConfig.setAllowCreate(false);
        envConfig.setTransactional(true);
        dbenv = new Environment(dbdir, envConfig);
        outputPath = context.getOutputUrl();

        // alocate multiple threads for dumping the data
        this.exporterThreadPool = new WorkerThreadPool(NUMBER_OF_THREADS, NUMBER_OF_JOBS, context.getRuleServiceProvider());
    }

    public void destroy()
    {
        try {
            dbenv.close();
        }
        catch (Exception e) { }
    }

    public void exportAll() throws Exception {
        // Version check
        String projectVersion = context.getComponentVersion();
        String dataVersion = getBdbVersion();
        final Logger logger = this.context.getLogger();
        logger.log(Level.INFO, "Project name: %s, Project version: BE %s, data version: BE %s",
                this.context.getProjectName(),
                this.context.getComponentVersion(),
                dataVersion);

        if (!projectVersion.substring(0, 2).equals(dataVersion.substring(0, 2))) {
            logger.log(Level.ERROR, "Can not use BE %s project to export data generated by BE %s project! exit.",
                    projectVersion,
                    dataVersion);
            return;
        }

        // Create CSV files for all concetps and concept properties
        createConceptfiles();

        try {
            exporterThreadPool.execute(new ExportPropIndexToPropInfo());
            exporterThreadPool.execute(new ExportEvents());
            exporterThreadPool.execute(new ExportConcepts());
        }
        catch (Exception ex) {
            ex.printStackTrace();
            throw new RuntimeException(ex);
        }

        // Wait for building propIndexToPropInfo and className HashMap are done as they are needed by exportConceptProperties
        exporterThreadPool.join();

       // Now exporting concept properties
        exportConceptProperties();
        cleanupCSVCeptWriter();
        exportScorecardIdsTable();
        writeControlFile(context, dataVersion);

        int numErr = stats.getErrorCount();
        int numWarn = stats.getWarningCount();
        if (numErr == 0 && numWarn == 0)
            logger.log(Level.INFO, "Exported all CSV files successfully.");
        else {
            logger.log(Level.INFO,
                    "Exported all CSV files with %d errors and %d warnings. Please check following lines for details.",
                    numErr,
                    numWarn);
            Iterator errors = stats.getErrors();
            while(errors.hasNext())
            	logger.log(Level.ERROR, (String) errors.next());

            Iterator warnings = stats.getWarnings();
            while(warnings.hasNext())
            	logger.log(Level.ERROR, (String) warnings.next());
        }
    }

    protected void exportPropIndexToPropInfo() throws Exception {
        this.context.getLogger().log(Level.INFO, "Exporting property index file...");
        DatabaseEntry keyEntry = new DatabaseEntry();
        DatabaseEntry dataEntry = new DatabaseEntry();
        Database propertyindex = dbenv.openDatabase(null, prop.getProperty("be.engine.om.berkeleydb.propertyindextable"), null);
        Cursor cursor = propertyindex.openCursor(null, null);

        CSVWriter piWriter = new CSVWriterImpl(outputPath, "propertiesIndex");
        piWriter.writeCommentln("conceptName,propertyName,propertyIndex");

        while(cursor.getNext(keyEntry, dataEntry, null) != OperationStatus.NOTFOUND) {
            DataInputStream keyis = new DataInputStream(new ByteArrayInputStream(keyEntry.getData()));
            DataInputStream datais = new DataInputStream(new ByteArrayInputStream(dataEntry.getData()));
            writePropIndexRow(keyis, datais, piWriter);
        }
        this.context.getLogger().log(Level.INFO, "Exported property index file.");
        piWriter.close();
        cursor.close();
        propertyindex.close();

    }

    private void writePropIndexRow(DataInputStream keyis, DataInputStream datais, CSVWriter writer) throws Exception {
        String subjectName, propName;
        String propClassName=keyis.readUTF();
        int index=datais.readInt();
        PropertyDefinition pd = null;
        int historySize = 0;

        if(propClassName.equals("$MarkerRecord$")) { // What is "$MarkerRecord$" for and what is the use here?
            subjectName = propName = "$MarkerRecord$";
        } else {
            String[] contents = DbUtils.EntityPathfromClass(propClassName).split("\\$\\$");
            subjectName = contents[0];
            propName = contents[1].substring(2); // Get rid of the 1z also.
            //this.context.getLogger().log(Level.INFO, "propName = %s", propName);
            if(propName.equals("TransitionStatuses")) {// From Code generation
                historySize = 2;
            }
            else {
                Concept c = context.getProject().getOntology().getConcept(subjectName);
                if(c == null) {
                    stats.addErrorString("Unknown Concept: " + subjectName);
                    return;
                }
                else {
                    pd = c.getPropertyDefinition(propName, false);
                    if(pd == null && (!propName.equals(DbUtils.TRANSITION_STATUSES))) {
                        stats.addErrorString("Unknown Property: " + propName + " of Concept: " + subjectName);
                        return;
                    }
                    else
                        historySize = pd.getHistorySize();
                }
            }
        }
        writer.write(subjectName);
        writer.write(propName).write(index);
        writer.writeln();

        PropertyInfo pi = new PropertyInfo();
        pi.propRdfIndex = index;
        pi.subjectName = subjectName;
        pi.propName = propName;
        pi.pd = pd;
        pi.historySize = historySize;
        propIndexToPropInfo.put(new Integer(index), pi);
    }

    private void writeControlFile(ExpImpContext context, String bdbVersion) throws Exception {
        CSVWriter controlWriter = new CSVWriterImpl(outputPath, "export-control");
        controlWriter.writeCommentln("project-name,project-config-version,data-version,lastInternalId,#instances,#events,#error,#warnings");
        controlWriter.write(context.getProjectName());
        controlWriter.write(context.getComponentVersion());
        controlWriter.write(bdbVersion);
        controlWriter.write(lastInternalId);
        controlWriter.write(stats.getInstanceCount());
        controlWriter.write(stats.getEventCount());
        controlWriter.write(stats.getErrorCount());
        controlWriter.write(stats.getWarningCount());
        controlWriter.writeln();
        controlWriter.close();
    }

    private String getBdbVersion() throws Exception {

        DatabaseEntry keyEntry = new DatabaseEntry();
        DatabaseEntry dataEntry = new DatabaseEntry();
        ByteArrayOutputStream bytes = new ByteArrayOutputStream();
        DataOutputStream dataos = new DataOutputStream(bytes);
        Database versiontable = dbenv.openDatabase(null, prop.getProperty("be.engine.om.berkeleydb.beversiontable"), null);

        dataos.writeUTF( BE_VERSION_RECORD);
        keyEntry.setData(bytes.toByteArray());
        versiontable.get(null, keyEntry, dataEntry, null);
        DataInputStream datais = new DataInputStream(new ByteArrayInputStream(dataEntry.getData()));

        String bdbVersion = datais.readUTF();
        return bdbVersion;
    }

    protected void createConceptfiles() throws Exception {

        // Creates files for concept and its property for each concept type in the project, regardless the existence of its data in BDB
        CSVWriter writers[] = null;
        String cptName =  null;
        String encoding = prop.getProperty("be.encoding");//TODO - Find the right encoding key
        Collection concepts = context.getProject().getOntology().getConcepts();
        int count = 0;

        final Logger logger = this.context.getLogger();
        logger.log(Level.INFO, "Creating %d concept and concept-properties files...",
                (concepts.size()*2+1));

        for(Iterator it = concepts.iterator(); it.hasNext();) {
            Concept c = (Concept) it.next();
            cptName = ModelNameUtil.modelPathToGeneratedClassName(c.getFullPath());

            logger.log(Level.DEBUG, "Creating concept and concept-properties files for: %s", cptName);

            writers = new CSVWriter[2];
            writers[0] = new CSVWriterImpl(outputPath, cptName);
            writers[0].writeCommentln("id,extId,status,timestamp,retractedFlag");

            writers[1] = new CSVWriterImpl(outputPath , cptName + "-properties");
            writers[1].writeCommentln("conceptId,propertyName,type,isSet,arrayIndex,value,historysize,currentIndex,[{HistoryTS,HistoryValue}...]");

            csvCeptWriters.put(cptName, writers);
            count = count + 2;
        }

        // Create concept file for URIRECORD
        logger.log(Level.DEBUG, "Creating concept file for: %s", DbUtils.URIRECORD_CLASS_STRING);
        writers = new CSVWriter[2];
        writers[0] = new CSVWriterImpl(outputPath, DbUtils.URIRECORD_CLASS_STRING);
        writers[0].writeCommentln("id,extId,status,timestamp,retractedFlag");
        writers[1] = null; // not to write properties file for URIRecord
        csvCeptWriters.put(DbUtils.URIRECORD_CLASS_STRING, writers);
        count++;

        logger.log(Level.INFO, "Created %d concept and concept-properties files.", count);
    }

    protected void exportConcepts() throws Exception {
        DatabaseEntry keyEntry = new DatabaseEntry();
        DatabaseEntry dataEntry = new DatabaseEntry();
        Database conceptDb = dbenv.openDatabase(null, prop.getProperty("be.engine.om.berkeleydb.conceptable"), null);
        Cursor cursor = conceptDb.openCursor(null, null);
        CSVWriter writers[] = null;

        final Logger logger = this.context.getLogger();
        logger.log(Level.INFO, "Exporting %d concepts...", conceptDb.count());

        while(cursor.getNext(keyEntry, dataEntry, null) != OperationStatus.NOTFOUND) {
            DataInputStream datais = new DataInputStream(new ByteArrayInputStream(dataEntry.getData()));
            EntityHeader header = new EntityHeader(datais);

            logger.log(Level.DEBUG, "Exporting concept: %s, id: %s", header.className, header.id);

            writers = (CSVWriter[]) csvCeptWriters.get(header.className);
            if (writers == null) {// this should not happen unless there are some changes in the project that runs with migration.
                stats.addErrorString("Concept: " + header.className + " is not in the project, ignored.");
                continue;
            }
            if(header.className.equals(DbUtils.URIRECORD_CLASS_STRING)) {
                lastInternalId = datais.readLong();
            }
            ceptClsNames.put(new Long(header.id), header.className);
            header.writeHeaderOnly((CSVWriter)writers[0]);
            stats.incrementInstances();
        }
        //writeConceptProperties();

        cursor.close();
        conceptDb.close();

        logger.log(Level.INFO, "Exported %d concepts.", this.stats.getInstanceCount());
    }

    private void exportConceptProperties() throws Exception {
    	DatabaseEntry keyEntry = new DatabaseEntry();
        DatabaseEntry dataEntry = new DatabaseEntry();
        Database propertiesDb = dbenv.openDatabase(null, prop.getProperty("be.engine.om.berkeleydb.propertiestable"), null);
        Cursor cursor = propertiesDb.openCursor(null, null);

        this.context.getLogger().log(Level.INFO, "Exporting %d concept properties...", propertiesDb.count());

        while(cursor.getNext(keyEntry, dataEntry, null) != OperationStatus.NOTFOUND) {
            DataInputStream keyis = new DataInputStream(new ByteArrayInputStream(keyEntry.getData()));
            DataInputStream datais = new DataInputStream(new ByteArrayInputStream(dataEntry.getData()));
            writePropertyRow(keyis, datais);
        }

        this.context.getLogger().log(Level.INFO, "Finished exporting concept properties.");

    }

    private void writePropertyRow(DataInputStream keyis, DataInputStream datais) throws Exception{

        long subjectId = keyis.readLong();   //subject id is the concept id
        int propIndex = keyis.readInt();   //propIndex
        PropertyInfo pi = (PropertyInfo)propIndexToPropInfo.get(new Integer(propIndex));
        if(pi == null) {
        	stats.addErrorString("Can not find PropertyInfo for propIndex: " + propIndex + " of concept internalId: " + subjectId);
        	return;
        }

        CSVWriter writers[] = null;
    	writers = (CSVWriter[]) csvCeptWriters.get(ceptClsNames.get(new Long(subjectId)));
    	if(writers == null || (writers != null && writers[1] == null)) {
    		stats.addErrorString("Can not find concept property: " + pi.propName+ " of concept internalId: " + subjectId);
    		return;
    	}

        byte propTypeIndex = datais.readByte(); //type of the Property //ignoring for the moment, since we already have the propertyType
        if (pi.pd != null) { // concept properties
            if (pi.pd.isArray()) {
                handlePropertyArray(subjectId, pi, datais, writers[1], propTypeIndex);
            }
            else {
                handlePropertyAtom(subjectId, pi, datais, -1, writers[1], propTypeIndex);
            }
        }
        else if(pi.pd == null && pi.propName.equals("TransitionStatuses")) {// statemachine states
            handlePropertyArray(subjectId, pi, datais, writers[1], propTypeIndex); // TransitionStatuses is always an array
        }
        else
            stats.addErrorString("Property: " + pi.propName + "does not have PropertyDefinition. Ignored.");
    }

    private void handlePropertyArray(long subjectId, PropertyInfo pi, DataInputStream is, CSVWriter cw, int propTypeIndex) throws Exception  {

        int arraySize = is.readInt();

        if(arraySize == 0) { // Special case for any 0 sized arrays that might exist in the db.
            cw.write(subjectId).write(pi.propName).write(propTypeIndex).write(false);
            // Write a single row with -1 as array index indicating this was a 0 sized array
        	if(pi.historySize == 0) // Handle 0 history case differently
            cw.write(-1).write(null).write(0).write(0);
            else {
            	cw.write(-1).write(null).write(pi.historySize).write(0);
                for(int i=0; i < pi.historySize; i++)
                    cw.write(0).write(null);
            }
        	cw.writeln();
            return;
        }

        for(int i=0; i < arraySize; i++) {
            handlePropertyAtom(subjectId, pi, is, i, cw, propTypeIndex);
        }
    }

    private void handlePropertyAtom(long subjectId, PropertyInfo pi, DataInputStream is, int arrayIndex, CSVWriter cw, int propTypeIndex) throws Exception {

	try {
        boolean isSet = is.readBoolean();
        int historySize = 0;
        int modelHistorySize = pi.pd != null ? pi.pd.getHistorySize() : pi.historySize;
        short currIndex = 0;
        long[] timeArray = null;
        String[] valueArray = null;
        String value = null;


        if(modelHistorySize != 0) { // Handle 0 history case differently
            historySize = is.readInt(); //history Size, should be equal to model size, but anyway it is written
            currIndex = is.readShort(); // History Index
            timeArray = new long[historySize];
            for (int i = 0; i < historySize; i++) {
                timeArray[i] = is.readLong(); // read all the time values;
            }

            valueArray = new String[historySize];
            for (int i = 0; i < historySize; i++) {
                valueArray[i] = readValue(is, propTypeIndex);
            }
            value = valueArray[currIndex];
        }
        else {
            value = readValue(is, propTypeIndex);
        }

        cw.write(subjectId);
        cw.write(pi.propName);
        cw.write(propTypeIndex);
        cw.write(isSet);
        cw.write(arrayIndex);
        cw.write(value);
        cw.write(historySize);
        cw.write(currIndex);
        for (int i=0; i < historySize; i++) {
            cw.write(timeArray[i]);
            cw.write(valueArray[i]);
        }
        cw.writeln();
	} catch (Exception e) {
		{
			this.context.getLogger().log(Level.INFO, "%s, %d, %d", pi.propName, arrayIndex, propTypeIndex);
			return;
		}
	}
    }

    private String readValue(DataInputStream is, int propTypeIndex) throws Exception{
        StringBuffer sb = new StringBuffer();
        switch(propTypeIndex) {
            case PropertyTypes.propertyTypes_atomBoolean:
            case PropertyTypes.propertyTypes_arrayBoolean:
                boolean value =  is.readBoolean();
                sb.append(value);

                break;
            case PropertyTypes.propertyTypes_atomConceptReference:
            case PropertyTypes.propertyTypes_atomContainedConcept:
            case PropertyTypes.propertyTypes_arrayConceptReference:
            case PropertyTypes.propertyTypes_arrayContainedConcept:
                if(is.readBoolean()) {
                    long valuel =  is.readLong();
                    sb.append(valuel);
                }
                else {
                    return null; //for reference or contained object it is null
                }
                break;

            case PropertyTypes.propertyTypes_atomLong:
            case PropertyTypes.propertyTypes_arrayLong:
                long valuel =  is.readLong();
                sb.append(valuel);
                break;

            case PropertyTypes.propertyTypes_atomDateTime:
            case PropertyTypes.propertyTypes_atomString:
            case PropertyTypes.propertyTypes_arrayDateTime:
            case PropertyTypes.propertyTypes_arrayString:
                if(is.readBoolean()) {
                    String value2 =  is.readUTF();
                    sb.append(value2);
                }
                else {
                    return null;
                }
                break;

            case PropertyTypes.propertyTypes_atomDouble:
            case PropertyTypes.propertyTypes_arrayDouble:
                double value3=  is.readDouble();
                sb.append( value3);

                break;

            case PropertyTypes.propertyTypes_atomInt:
            case PropertyTypes.propertyTypes_arrayInt:
                int value4=  is.readInt();
                sb.append( value4);
                break;
            default:
                stats.addErrorString(propTypeIndex + ", Unknown property Index seen");


        }

        return sb.toString();
    }

    protected void exportScorecardIdsTable() throws Exception {
        DatabaseEntry keyEntry = new DatabaseEntry();
        DatabaseEntry dataEntry = new DatabaseEntry();
        Database namedinstances = dbenv.openDatabase(null, prop.getProperty("be.engine.om.berkeleydb.namedinstancemap", "BENamedInstanceIds"), null);
        Cursor cursor = namedinstances.openCursor(null, null);

        CSVWriter scWriter = new CSVWriterImpl(outputPath, "scorecardIds");
        scWriter.writeCommentln("scorecardName,scorecardId");
        while(cursor.getNext(keyEntry, dataEntry, null) != OperationStatus.NOTFOUND) {
            DataInputStream keyis = new DataInputStream(new ByteArrayInputStream(keyEntry.getData()));
            DataInputStream datais = new DataInputStream(new ByteArrayInputStream(dataEntry.getData()));
            String scorecardName = DbUtils.EntityPathfromClass(keyis.readUTF());
            long scorecardId = datais.readLong();
            //this.scoreCardSet.add(new Long(scorecardId));
            scWriter.write(scorecardName).write(scorecardId);
            scWriter.writeln();
        }

        scWriter.close();
        cursor.close();
        namedinstances.close();

    }

    protected void exportEventLog() throws Exception{

        DatabaseEntry keyEntry = new DatabaseEntry();
        DatabaseEntry dataEntry = new DatabaseEntry();
        Database eventstable = dbenv.openDatabase(null, prop.getProperty("be.engine.om.berkeleydb.eventslog"), null);
        Cursor cursor = eventstable.openCursor(null, null);

        final Logger logger = this.context.getLogger();
        logger.log(Level.INFO, "Number of events including historical checkpoint events is %d", eventstable.count());

        Iterator propdefs = null;
        // Create event and payload files for each event type, regardless the existence of the event and payload data
        CSVWriter writers[] = null;
        StringBuffer hdRow = hdRow = new StringBuffer("id,extId,status,timestamp,retractedFlag");
        Collection events = context.getProject().getOntology().getEvents();
        for(Iterator it = events.iterator(); it.hasNext();) {
            Event e = (Event) it.next();
            String className = ModelNameUtil.modelPathToGeneratedClassName(e.getFullPath());
            if(!className.equals("com.tibco.be.engine.model.event.Impl.CheckpointEvent")) { // Not to create file for CheckpointEvent
	            propdefs = e.getUserProperties();
	            while(propdefs.hasNext()) {
	                EventPropertyDefinition pd = (EventPropertyDefinition)propdefs.next();
	                hdRow.append(",").append(pd.getPropertyName());
	            }
	            writers = new CSVWriter[2];
	            writers[0] = new CSVWriterImpl(outputPath, className);
	            writers[0].writeCommentln(hdRow.toString());
	            if(e.getType() == Event.SIMPLE_EVENT) {
	            	writers[1] = new CSVWriterImpl(outputPath, className + "-payload");
	                writers[1].writeCommentln("eventId,payloadType,payloadData");
	            }
	            else writers[1] = null;
	            csvEvtWriters.put(className,writers);
            }
        }

        long numCPEvents = 0;
        while(cursor.getNext(keyEntry, dataEntry, null) != OperationStatus.NOTFOUND) {
            DataInputStream datais = new DataInputStream(new ByteArrayInputStream(dataEntry.getData()));
            EntityHeader header = new EntityHeader (datais);

            if(!header.className.equals("com.tibco.be.engine.model.event.Impl.CheckpointEvent")) { // Not to export CheckpointEvent data
                String eventPath=DbUtils.EntityPathfromClass(header.className);
                Event e = context.getProject().getOntology().getEvent(eventPath);
                String encoding = prop.getProperty("be.encoding");//TODO - Find the right encoding key

                logger.log(Level.DEBUG, "Exporting event: %s, id: %d", header.className, header.id);

                writers = (CSVWriter[])csvEvtWriters.get(header.className);
                if(writers == null) {
                	stats.addErrorString("Event: " + header.className + " is not in the project, ignored.");
                	continue;
                }
                evtClsNames.put(new Long(header.id), header.className);
                header.writeHeader((CSVWriter)writers[0]);

                //Writer event properties
                propdefs = e.getUserProperties();
                while(propdefs.hasNext()) {
                    EventPropertyDefinition pd = (EventPropertyDefinition) propdefs.next();
                    writeEventProperty((CSVWriter)writers[0], RDFTypes.getIndex(pd.getType()), datais);
                }
                writers[0].writeln();
                stats.incrementsEvents();
            } else {
                numCPEvents++;
            }
        }
        //Write event playload
        writeEventPayload();

        for(Iterator it=csvEvtWriters.values().iterator();it.hasNext();) {
            writers = (CSVWriter[])it.next();
            writers[0].close();
            if(writers[1] != null) writers[1].close();
        }
        csvEvtWriters.clear();
        cursor.close();
        eventstable.close();

        logger.log(Level.INFO,
                "Exported %d events. %d historical checkpoint events are ignored as they won't be migrated.",
                this.stats.getEventCount(),
                numCPEvents);
    }

    private void writeEventProperty(CSVWriter writer, int rdftype, DataInputStream is) throws Exception {
        switch(rdftype) {

        case RDFTypes.STRING_TYPEID :
        case RDFTypes.DATETIME_TYPEID :
            String strval;
            if(is.readBoolean())
                strval=is.readUTF();
            else
                strval=null;
            writer.write(strval);
            break;
        case RDFTypes.INTEGER_TYPEID :
            int intval=is.readInt();
            writer.write(intval);
            break;
        case RDFTypes.LONG_TYPEID :
        case RDFTypes.CONCEPT_TYPEID :
        case RDFTypes.CONCEPT_REFERENCE_TYPEID :
            long longval = is.readLong();
            writer.write(longval);
            break;
        case RDFTypes.DOUBLE_TYPEID :
            double dblval=is.readDouble();
            writer.write(dblval);
            break;
        case RDFTypes.BOOLEAN_TYPEID :
            boolean boolval=is.readBoolean();
            writer.write(boolval ? 1 : 0);
            break;
        default:
            stats.addErrorString(RDFTypes.driverTypeStrings[rdftype] + ", Unknown property type seen");
        }
    }

    private void writeEventPayload() throws Exception {
    	DatabaseEntry keyEntry = new DatabaseEntry();
    	DatabaseEntry dataEntry = new DatabaseEntry();
        Database payloads = dbenv.openDatabase(null, "BEEventPayloads", null);
        Cursor cursor = payloads.openCursor(null, null);

        String encoding = prop.getProperty("be.encoding");//TODO - Find the right encoding key

        while(cursor.getNext(keyEntry, dataEntry, null) != OperationStatus.NOTFOUND) {
            DataInputStream keyis = new DataInputStream(new ByteArrayInputStream(keyEntry.getData()));
            DataInputStream datais = new DataInputStream(new ByteArrayInputStream(dataEntry.getData()));
            writePayloadRow(keyis, datais);
        }

        evtClsNames.clear();
        cursor.close();
        payloads.close();
    }

    private void writePayloadRow(DataInputStream keyis, DataInputStream datais) throws Exception {

        long eventid = keyis.readLong();
        // Changed to handle different payload types.
        int payloadtype = datais.readInt(); //TODO: need to figure out what payloadtype int value stands for
        byte[] payloadbytes;
        if(payloadtype > 0) { // = payload length already
            payloadbytes = new byte[payloadtype];
            datais.read(payloadbytes);
        } else {
            int len = datais.readInt();
            payloadtype = len;
            payloadbytes = new byte[len];
            datais.read(payloadbytes);
        }
        //this.context.getLogger().log(Level.INFO, "Payload string: %s", new String(payloadbytes));

        String eventName = (String)evtClsNames.get(new Long(eventid));
        if(eventName == null)
        	stats.addErrorString("Can not find corresponding event name for eventid: " + eventid + " from payload! Ignore this payload."); //This should not happen.
        else {
	        CSVWriter[] writers = (CSVWriter[])csvEvtWriters.get(eventName);
	        if(writers == null) {
	        	stats.addErrorString("Can not find corresponding event payload writer for event " + eventName + "! Create a new writer."); //This should not happen.
	        	writers[0] = null;
	        	writers[1] = new CSVWriterImpl(outputPath, eventid + "-payload"); //this file won't be loaded in case of payload does not have a host event as the file name is different.
	            writers[1].writeCommentln("eventId,payloadType,payloadData");
	        }
	        writers[1].write(eventid).write(payloadtype);
	        writers[1].write(Base64.encodeBytes(payloadbytes));
	        //writers[1].write(new String(payloadbytes));
	        writers[1].writeln();
        }
    }

    protected void cleanupCSVCeptWriter() {
        CSVWriter writers[] = null;

        try {
            for(Iterator it=csvCeptWriters.values().iterator();it.hasNext();) {
                writers = (CSVWriter[])it.next();
                writers[0].close();
                if(writers[1] != null)
                    writers[1].close();
            }
            csvCeptWriters.clear();
         }
        catch (Exception ex) {
            ex.printStackTrace();
        }
    }

    private static String massageNameSpaceString(String ns) {
        if(ns == null)
            return DbUtils.ROOT_TABLENAMESPACE;
        ns = DbUtils.ROOT_TABLENAMESPACE + ns;
        while(ns.length() > 0 && ns.charAt(ns.length()-1) == '/')
            ns = ns.substring(0,ns.length()-1); // Trim the trailing /
        ns = ns.replace('/', '.'); // Now remove any / in the path and convert them to .
        return ns;
    }
    protected class ExportPropIndexToPropInfo implements Runnable {
        public void run() {
            Thread.currentThread().setName("PropIndexToPropInfo.ExportThread");

            try {
                exportPropIndexToPropInfo();
            }
            catch (Exception ex) {
                ex.printStackTrace();
                throw new RuntimeException(ex);
            }

            exporterThreadPool.afterExecute();
        }
    }

    protected class ExportEvents implements Runnable {
        public void run() {
            Thread.currentThread().setName("Events.ExportThread");

            try {
                exportEventLog();
            }
            catch (Exception ex) {
                ex.printStackTrace();
                throw new RuntimeException(ex);
            }

            exporterThreadPool.afterExecute();
        }
    }

    protected class ExportConcepts implements Runnable {
        public void run() {
            Thread.currentThread().setName("Concepts.ExportThread");

            try {
                exportConcepts();
            }
            catch (Exception ex) {
                ex.printStackTrace();
                throw new RuntimeException(ex);
            }

            exporterThreadPool.afterExecute();
        }
    }

    private static class EntityHeader {

        boolean retractedFlag; // Concept/Event marked deleted? Part of status too.
        String className; // Concept/Event class name
        long timeStamp; // Time of add/delete -- introduced in 1.1
        long id; // Internal Id
        String extId; //External Id
        byte status; // Status Flags

        EntityHeader(DataInputStream is) throws IOException {
            retractedFlag = is.readBoolean();
            className = is.readUTF();
            timeStamp = is.readLong();
            id = is.readLong();
            if(is.readBoolean())
                extId = is.readUTF();
            else
                extId = null;
            status = is.readByte();
        }

        /**
         * Writes the entity header and marks the end of the line.
         */
        public void writeHeaderOnly(CSVWriter cw) throws IOException {

            cw.write(id).write(extId).write(status).write(timeStamp).write(retractedFlag);
            cw.writeln();
        }

        /**
         * Writes the entity header without marking the end of the line.
         */
        public void writeHeader(CSVWriter cw) throws IOException {

            cw.write(id).write(extId).write(status).write(timeStamp).write(retractedFlag);
        }
    }

    private static class PropertyInfo {
        public String propName;
        public int propRdfIndex;
        public int historySize;
        public String subjectName;
        public PropertyDefinition pd;
    }

}
